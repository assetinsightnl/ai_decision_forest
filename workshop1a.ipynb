{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(predictor_columns, shuffle_data, train_size, random_seed=0):\n",
    "    \"\"\"Read the dataset from the CSV file and prepare the data for using it in training and testing the neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictor_columns : list of strings\n",
    "        The names of the columns used for predicting the classes (the Iris species)\n",
    "    shuffle_data : bool\n",
    "        Whether or not the data is shuffled before using it for the neural network\n",
    "    train_size : int\n",
    "        The number of rows in the dataset of 150 rows in total which will be used for training the\n",
    "        network. The remainder of the rows will be used for testing the trained network.\n",
    "    random_seed : int\n",
    "        Seed for data shuffling.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    iris_classes : list of strings\n",
    "        List of Iris species.\n",
    "    x_testset : numpy array of floats\n",
    "        Test dataset.\n",
    "    x_trainset : numpy array of floats\n",
    "        Train dataset.\n",
    "    y_testset : numpy array of floats\n",
    "        Labels test dataset, one hot encoded.\n",
    "    y_trainset : numpy array of floats\n",
    "        Labels train dataset, one hot encoded.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the dataset from the file\n",
    "    dataset = pd.read_csv('iris_data.csv')\n",
    "    iris_classes = dataset['Species'].unique()  # Create a list with all Iris species\n",
    "    dataset = pd.get_dummies(dataset, columns=['Species'])  # One Hot encoding\n",
    "\n",
    "    # Select the correct classification and predictor columns and make numpy arrays of them with the correct types\n",
    "    # Classification columns: 'Species_Iris-setosa', 'Species_Iris-versicolor', 'Species_Iris-virginica'\n",
    "    y = dataset[list(dataset.columns.values)[-3:]]\n",
    "    y = np.array(y, dtype='float32')\n",
    "\n",
    "    # Predictor columns: (a subset of) 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'\n",
    "    x = dataset[predictor_columns]\n",
    "    x = np.array(x, dtype='float32')\n",
    "\n",
    "    # Shuffle the data; initially the data is ordered based on their class\n",
    "    if shuffle_data:\n",
    "        np.random.seed(random_seed)\n",
    "        indices = np.random.choice(len(x), len(x), replace=False)\n",
    "        x_values, y_values = x[indices], y[indices]\n",
    "    else:\n",
    "        x_values, y_values = x, y\n",
    "\n",
    "    # Create a train set and a test set. Total dataset size is 150.\n",
    "    test_size = 150 - train_size\n",
    "    x_testset = x_values[-test_size:]\n",
    "    x_trainset = x_values[:-test_size]\n",
    "    y_testset = y_values[-test_size:]\n",
    "    y_trainset = y_values[:-test_size]\n",
    "\n",
    "    return iris_classes, x_testset, x_trainset, y_testset, y_trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter values\n",
    "# Play with these parameters to improve results!\n",
    "\n",
    "# Columns used to predict the Iris-classes. All columns: SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm.\n",
    "predictor_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm']\n",
    "\n",
    "# Whether or not the input data is be shuffled. Initially, the first 50 rows are Iris Setosa, the next 50 rows are\n",
    "# Iris Versicolor and the last 50 rows are Iris Virginica.\n",
    "shuffle_data = False\n",
    "\n",
    "# Number of rows to be used for training the network. In total there are 150 rows in the dataset. The remaining rows\n",
    "# will be used to test the trained network.\n",
    "train_size = 75\n",
    "\n",
    "# The maximum depth of the decision tree(s).\n",
    "max_depth = 1\n",
    "\n",
    "# Number of trees in the random forest.\n",
    "n_forest_estimators = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test a single decision tree\n",
    "\n",
    "# Get data\n",
    "iris_classes, x_testset, x_trainset, y_testset, y_trainset = get_data(predictor_columns, shuffle_data, train_size)\n",
    "\n",
    "# Make decision tree model\n",
    "tree = DecisionTreeClassifier(random_state=0, max_depth=max_depth)\n",
    "\n",
    "# Train model\n",
    "tree.fit(x_trainset, y_trainset)\n",
    "\n",
    "# Run on test set\n",
    "acc = tree.score(x_testset, y_testset)\n",
    "\n",
    "# Print results\n",
    "print(\"REPORT\")\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"Predictor columns: \" + str(predictor_columns))\n",
    "print(\"Shuffle data: \" + str(shuffle_data))\n",
    "print(\"Training dataset size: \" + str(train_size))\n",
    "print(\"Max depth: \" + str(max_depth))\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"Tree test accuracy: \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export decision tree graph\n",
    "\n",
    "graph = export_graphviz(tree, feature_names=predictor_columns, impurity=False, node_ids=False)\n",
    "print(\"Copy and paste code below to http://www.webgraphviz.com/ to see tree structure\")\n",
    "print(\"Read the 'value' field as follows:\")\n",
    "print(\"[[# not class1, # class1], [# not class2, # class2], [# not class3, # class3]]\")\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test a random forest\n",
    "\n",
    "# Get data\n",
    "iris_classes, x_testset, x_trainset, y_testset, y_trainset = get_data(predictor_columns, shuffle_data, train_size)\n",
    "\n",
    "# Make random forest model\n",
    "forest = RandomForestClassifier(random_state=0, max_depth=max_depth, n_estimators=n_forest_estimators)\n",
    "\n",
    "# Train model\n",
    "forest.fit(x_trainset, y_trainset)\n",
    "\n",
    "# Run on test set\n",
    "acc = forest.score(x_testset, y_testset)\n",
    "\n",
    "# Print results\n",
    "print(\"REPORT\")\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"Columns: \" + str(predictor_columns))\n",
    "print(\"Shuffle data: \" + str(shuffle_data))\n",
    "print(\"Training dataset size: \" + str(train_size))\n",
    "print(\"Max depth: \" + str(max_depth))\n",
    "print(\"Number of estimators in forest: \" + str(n_forest_estimators))\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"Forest test accuracy: \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
